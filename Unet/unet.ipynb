{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import *\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import mia_datasets_tf as mia_data\n",
    "import mia_evaluation as mia_eval\n",
    "import mia_losses_tf as mia_losses\n",
    "import mia_utils\n",
    "import SimpleITK as sitk\n",
    "import os \n",
    "import shutil\n",
    "import inspect\n",
    "import random\n",
    "\n",
    "sitk.ProcessObject_SetGlobalWarningDisplay(False)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = r\"/ssd2/jupyter/MIA/simple_unet\"\n",
    "\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model_creator, fit_params, num_runs, directory, name):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = None\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    valid_fit_args = inspect.signature(tf.keras.Model.fit).parameters.keys()\n",
    "    filtered_fit_params = {key: value for key, value in fit_params.items() if key in valid_fit_args}\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Training run {run + 1}/{num_runs}\\n\")\n",
    "        model = model_creator()\n",
    "\n",
    "        run_model_path = os.path.join(directory, f\"{name}_{run}.keras\")\n",
    "\n",
    "        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=run_model_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "        if \"callbacks\" in filtered_fit_params:\n",
    "            filtered_fit_params['callbacks'].append(checkpoint_cb)\n",
    "        else:\n",
    "            filtered_fit_params['callbacks'] = [checkpoint_cb]\n",
    "    \n",
    "        history = model.fit(\n",
    "            fit_params['training_data'],\n",
    "            **filtered_fit_params)\n",
    "\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_losses.append(history.history['val_loss'])\n",
    "\n",
    "        print(f\"\\nRun {run + 1} best val_loss = {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f\"New best model found\")\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = run_model_path\n",
    "\n",
    "    print(f\"\\n Best model: {best_model_path} with val_loss = {best_val_loss:.6f}\")\n",
    "\n",
    "    return best_model_path, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(images : list[np.ndarray], titles : list[str] | None):\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    \n",
    "    for index, image in enumerate(images):\n",
    "        axs[index].imshow(image[image.shape[0] // 2])\n",
    "        if titles is not None:\n",
    "            axs[index].set_title(titles[index])\n",
    "        axs[index].axis('off')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history, y_lim=(0.0, 1.5)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, run_val_loss in enumerate(history):\n",
    "        plt.plot(run_val_loss, label=f'Run {i + 1}')\n",
    "\n",
    "    plt.title('Validation Loss Across Runs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.ylim(y_lim)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=2141):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(255, 60, 60), (140, 255, 140), (140, 200, 255)] \n",
    "custom_cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inputs, n_filters=32, max_pooling=True):\n",
    "    layer = Conv3D(n_filters, 3, activation='relu', padding='same')(inputs)\n",
    "    layer = Conv3D(n_filters, 3, activation='relu', padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2, 2, 2))(layer)    \n",
    "    else:\n",
    "        next_layer = layer\n",
    "        \n",
    "    skip_connection = layer    \n",
    "    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def decoder_block(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "    up = Conv3DTranspose(n_filters, (3, 3, 3), strides=(2, 2, 2), padding='same')(prev_layer_input)\n",
    "    merge = concatenate([up, skip_layer_input], axis=4)\n",
    "\n",
    "    conv = Conv3D(n_filters, 3,  activation='relu', padding='same')(merge)\n",
    "    conv = Conv3D(n_filters, 3, activation='relu', padding='same')(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def unet_compiled(input_size, n_filters=32, n_classes=3, learning_rate=0.001):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    cblock1 = encoder_block(inputs, n_filters, max_pooling=True)\n",
    "    cblock2 = encoder_block(cblock1[0], n_filters * 2, max_pooling=True)\n",
    "    cblock3 = encoder_block(cblock2[0], n_filters * 4, max_pooling=True)\n",
    "    cblock4 = encoder_block(cblock3[0], n_filters * 8, max_pooling=True)\n",
    "    cblock5 = encoder_block(cblock4[0], n_filters * 16, max_pooling=False) \n",
    "    \n",
    "    ublock6 = decoder_block(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = decoder_block(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = decoder_block(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = decoder_block(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    conv9 = Conv3D(n_filters, 3, activation='relu', padding='same',)(ublock9)\n",
    "    conv10 = Conv3D(n_classes, 1, activation=\"Softmax\", padding='same', )(conv9)\n",
    "   \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iseg 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iseg2019_training = mia_data.Iseg2019Processed(r'/ssd2/jupyter/MIA/split_datasets/iseg2019_ns/training')\n",
    "iseg2019_validation = mia_data.Iseg2019Processed(r'/ssd2/jupyter/MIA/split_datasets/iseg2019_ns/validation')\n",
    "iseg2019_testing = mia_data.Iseg2019Processed(r'/ssd2/jupyter/MIA/split_datasets/iseg2019_ns/testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_specific_directory = os.path.join(model_directory, \"iseg2019_ns\")\n",
    "if not os.path.exists(dataset_specific_directory):\n",
    "    os.makedirs(dataset_specific_directory)\n",
    "\n",
    "inference_directory = os.path.join(dataset_specific_directory, \"inference\")\n",
    "if not os.path.exists(inference_directory):\n",
    "    os.makedirs(inference_directory)\n",
    "\n",
    "labels_dictionary = {\"CSF\" : 1, \"GM\" : 2, \"WM\" : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = iseg2019_training.subjects[0].get_T1()\n",
    "t2 = iseg2019_training.subjects[0].get_T2()\n",
    "label = iseg2019_training.subjects[0].get_label()\n",
    "\n",
    "plot_side_by_side([sitk.GetArrayViewFromImage(t1), sitk.GetArrayViewFromImage(t2), sitk.GetArrayViewFromImage(label)], [\"T1\", \"T2\", \"Label\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 + T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(dataset_specific_directory, \"iseg2019_T1T2.keras\")\n",
    "\n",
    "training = iseg2019_training.T1_T2_dataset()\n",
    "validation = iseg2019_validation.T1_T2_dataset()\n",
    "testing = iseg2019_testing.T1_T2_dataset()\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "input_shape = next(training.take(1).as_numpy_iterator())[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_creator = lambda : unet_compiled(input_size=input_shape,\n",
    "                                       n_filters=16,\n",
    "                                         n_classes=len(labels_dictionary) + 1,\n",
    "                                           learning_rate=0.00001)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50),\n",
    "]\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\" : callbacks_list,\n",
    "    \"training_data\" : training.batch(batch_size),\n",
    "    \"validation_data\" : validation.batch(batch_size),\n",
    "    'epochs' : 500\n",
    "}\n",
    "\n",
    "seed_all()\n",
    "best_model_path, history = run_training(unet_creator, fit_params, 10, dataset_specific_directory, \"iseg2019_T1T2\")\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    os.remove(model_file)\n",
    "    \n",
    "shutil.copy(best_model_path, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = keras.saving.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = unet.predict(testing.batch(1))\n",
    "predicted_labels = prediction.argmax(axis=-1)\n",
    "\n",
    "mia_utils.writeImagesArray(predicted_labels, inference_directory, lambda x : f\"T1_T2{x}.nii.gz\", lambda x : iseg2019_testing.subjects[x].get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(predicted_labels[0], (1,3), \"Iseg 2019 T1 + T2 Inference\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(next(testing.take(1).as_numpy_iterator())[1], (1,3), title=\"Iseg 2019 T1+T2 Ground truth\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = sitk.GetImageFromArray(predicted_labels[0])\n",
    "truth_image = sitk.GetImageFromArray(testing.as_numpy_iterator().next()[1])\n",
    "\n",
    "eval = mia_eval.evaluateImage(predicted_image, truth_image, labels_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_eval.createRecord(\"iseg2019\", eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONBID-HIE 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bondid2023_training = mia_data.BONDID2023Processed(r'/ssd2/jupyter/MIA/split_datasets/bonbid2023_ns/training', \"nii.gz\")\n",
    "bondid2023_validation = mia_data.BONDID2023Processed(r'/ssd2/jupyter/MIA/split_datasets/bonbid2023_ns/validation',  \"nii.gz\")\n",
    "bondid2023_testing = mia_data.BONDID2023Processed(r'/ssd2/jupyter/MIA/split_datasets/bonbid2023_ns/testing',  \"nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_specific_directory = os.path.join(model_directory, \"bonbid2023_ns\")\n",
    "if not os.path.exists(dataset_specific_directory):\n",
    "    os.makedirs(dataset_specific_directory)\n",
    "\n",
    "inference_directory = os.path.join(dataset_specific_directory, \"inference\")\n",
    "if not os.path.exists(inference_directory):\n",
    "    os.makedirs(inference_directory)\n",
    "\n",
    "labels_dictionary = {\"Lesion\" : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adc = bondid2023_training.subjects[0].get_ADC_ss()\n",
    "z_adc = bondid2023_training.subjects[0].get_Z_ADC()\n",
    "label = bondid2023_training.subjects[0].get_label()\n",
    "\n",
    "plot_side_by_side([sitk.GetArrayViewFromImage(adc), sitk.GetArrayViewFromImage(z_adc), sitk.GetArrayViewFromImage(label)], [\"ADC_ss\", \"Z_ADC\", \"Label\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC + Z_ADC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(dataset_specific_directory, \"bonbid2023_adc_zadc.keras\")\n",
    "\n",
    "training = bondid2023_training.ADC_ss_Z_ADC_dataset()\n",
    "testing = bondid2023_testing.ADC_ss_Z_ADC_dataset()\n",
    "validation = bondid2023_validation.ADC_ss_Z_ADC_dataset()\n",
    "batch_size = 2\n",
    "\n",
    "input_shape = next(training.take(1).as_numpy_iterator())[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
    "    keras.callbacks.ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, mode='min')\n",
    "]\n",
    "\n",
    "unet_creator = lambda : unet_compiled(input_size=input_shape, n_filters=16, n_classes=len(labels_dictionary) + 1, learning_rate=0.000001)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50),\n",
    "]\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\" : callbacks_list,\n",
    "    \"training_data\" : training.batch(batch_size),\n",
    "    \"validation_data\" : validation.batch(batch_size),\n",
    "    'epochs' : 500\n",
    "}\n",
    "\n",
    "seed_all()\n",
    "best_model_path, history = run_training(unet_creator, fit_params, 10, dataset_specific_directory, \"bonbid2023_adc_zadc\")\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    shutil.rmtree(model_file)\n",
    "    \n",
    "shutil.copy(best_model_path, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = keras.saving.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = unet.predict(testing.batch(1))\n",
    "predicted_labels = prediction.argmax(axis=-1)\n",
    "mia_utils.writeImagesArray(predicted_labels, inference_directory,\n",
    "                            lambda x : f\"ADC_Z_ADC{x}.nii.gz\",\n",
    "                            lambda x: bondid2023_testing.subjects[x].get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(predicted_labels[0], (0,1), \"Bondid 2023 ADC + Z_ADC Inference\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(next(testing.take(4).as_numpy_iterator())[1], (0,1), title=\"Bondid 2023 ADC + Z_ADC Ground truth\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = sitk.GetImageFromArray(predicted_labels[0])\n",
    "truth_image = sitk.GetImageFromArray(testing.as_numpy_iterator().next()[1])\n",
    "\n",
    "eval = mia_eval.evaluateImage(predicted_image, truth_image, labels_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_eval.createRecord(\"bondid2023\", eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TACR 6 HIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacrhie_training = mia_data.TACRHIE6Dataset(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/training', \"nii.gz\")\n",
    "tacrhie_validation = mia_data.TACRHIE6Dataset(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/validation',  \"nii.gz\")\n",
    "tacrhie_testing = mia_data.TACRHIE6Dataset(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/testing',  \"nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (128, 128, 128)\n",
    "\n",
    "training = mia_data.CroppedDataset(tacrhie_training.aseg_dataset(), target_shape).dataset()\n",
    "validation = mia_data.CroppedDataset(tacrhie_validation.aseg_dataset(), target_shape).dataset()\n",
    "testing = mia_data.CroppedDataset(tacrhie_testing.aseg_dataset(), target_shape).dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_specific_directory = os.path.join(model_directory, \"tacrhie\", \"aseg\")\n",
    "os.makedirs(dataset_specific_directory, exist_ok=True)\n",
    "\n",
    "inference_directory = os.path.join(dataset_specific_directory, \"inference\")\n",
    "os.makedirs(inference_directory, exist_ok=True)\n",
    "\n",
    "with open(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/aseg_labels.json') as file:\n",
    "    labels_dictionary = json.load(file)\n",
    "    del labels_dictionary['0']\n",
    "\n",
    "    labels_dictionary = {v : float(k) for k,v in labels_dictionary.items()}\n",
    "\n",
    "with open(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/aseg_colors.json') as file:\n",
    "    colours_dictionary = json.load(file)\n",
    "    del colours_dictionary['0']\n",
    "        \n",
    "    custom_cmap = ListedColormap([np.array(colours_dictionary[str(k)][:3]) / 255.0 for k in sorted(int(a) for a in colours_dictionary.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tacrhie_training.subjects[0].get_norm()\n",
    "aseg = tacrhie_training.subjects[0].get_aseg()\n",
    "aseg_aparc = tacrhie_training.subjects[0].get_aseg_aparc()\n",
    "\n",
    "plot_side_by_side([sitk.GetArrayViewFromImage(norm), sitk.GetArrayViewFromImage(aseg), sitk.GetArrayViewFromImage(aseg_aparc)], [\"norm\", \"aseg\", \"aseg+aparc\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aseg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(dataset_specific_directory, \"tacrhie6_aseg.keras\")\n",
    "\n",
    "num_channels = 1\n",
    "batch_size = 2\n",
    "\n",
    "input_shape = (*next(training.take(1).as_numpy_iterator())[0].shape, num_channels)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_creator = lambda : unet_compiled(input_size=input_shape, n_filters=32, n_classes=len(labels_dictionary) + 1, learning_rate=0.00001)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50),\n",
    "]\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\" : callbacks_list,\n",
    "    \"training_data\" : training.batch(batch_size),\n",
    "    \"validation_data\" : validation.batch(batch_size),\n",
    "    'epochs' : 500\n",
    "}\n",
    "\n",
    "seed_all()\n",
    "best_model_path, history = run_training(unet_creator, fit_params, 10, dataset_specific_directory, \"tacrhie_aseg\")\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    os.remove(model_file)\n",
    "    \n",
    "shutil.copy(best_model_path, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = keras.saving.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = unet.predict(testing.batch(1))\n",
    "predicted_labels = prediction.argmax(axis=-1).astype(np.uint8)\n",
    "\n",
    "shape = (predicted_labels.shape[0], 256, 256, 256)\n",
    "begin = (np.array(shape) - predicted_labels.shape) // 2\n",
    "begin[0] = 0\n",
    "\n",
    "results = mia_utils.embed_tensor(predicted_labels, shape, begin)\n",
    "\n",
    "mia_utils.writeImagesArray(results, inference_directory, \n",
    "                           lambda x : f\"{tacrhie_testing.subjects[x].number}.nii.gz\",\n",
    "                            lambda x : tacrhie_testing.subjects[x].get_aseg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "\n",
    "for index, img in enumerate(testing):\n",
    "    prediction = unet.predict(img)\n",
    "    predicted_labels = tf.math.argmax(prediction, -1).numpy()\n",
    "    shape = (predicted_labels.shape[0], 256, 256, 256)\n",
    "    begin = (np.array(shape) - predicted_labels.shape) // 2\n",
    "    begin[0] = 0\n",
    "\n",
    "    metrics = mia_eval.evaluateImage(sitk.GetImageFromArray(predicted_labels), sitk.GetImageFromArray(img[1].numpy()), labels_dictionary)\n",
    "    evaluations.append((str(index), metrics))\n",
    "    \n",
    "    mia_utils.writeImageArray(predicted_labels, \n",
    "                              os.path.join(inference_directory, f\"ADC_Z_ADC_{index}.nii.gz\"),\n",
    "                               lambda x: bondid2023_testing.subjects[x].get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(predicted_labels[0], (1,256), title=\"TACR-HIE aseg Inference\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_array = next(testing.take(1).as_numpy_iterator())[1]\n",
    "\n",
    "mia_utils.interactive_display(truth_array, (1,256), title=\"TACR-HIE aseg Ground truth\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = sitk.GetImageFromArray(predicted_labels[0])\n",
    "truth_image = sitk.GetImageFromArray(truth_array)\n",
    "\n",
    "eval = mia_eval.evaluateImage(predicted_image, truth_image, labels_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_eval.createRecord(\"tarc_aseg\", eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aseg + aparc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (128, 128, 128)\n",
    "\n",
    "training = mia_data.CroppedDataset(tacrhie_training.aseg_aparc_dataset(), target_shape).dataset()\n",
    "validation = mia_data.CroppedDataset(tacrhie_validation.aseg_aparc_dataset(), target_shape).dataset()\n",
    "testing = mia_data.CroppedDataset(tacrhie_testing.aseg_aparc_dataset(), target_shape).dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_specific_directory = os.path.join(model_directory, \"tacrhie\", \"aseg_aparc\")\n",
    "os.makedirs(dataset_specific_directory, exist_ok=True)\n",
    "\n",
    "inference_directory = os.path.join(dataset_specific_directory, \"inference\")\n",
    "os.makedirs(inference_directory, exist_ok=True)\n",
    "\n",
    "with open(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/aseg_aparc_labels.json') as file:\n",
    "    labels_dictionary = json.load(file)\n",
    "    del labels_dictionary['0']\n",
    "\n",
    "    labels_dictionary = {v : float(k) for k,v in labels_dictionary.items()}\n",
    "\n",
    "with open(r'/ssd2/jupyter/MIA/split_datasets/tacrhie/aseg_aparc_colors.json') as file:\n",
    "    colours_dictionary = json.load(file)\n",
    "    del colours_dictionary['0']\n",
    "        \n",
    "    custom_cmap = ListedColormap([np.array(colours_dictionary[str(k)][:3]) / 255.0 for k in sorted(int(a) for a in colours_dictionary.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tacrhie_training.subjects[0].get_norm()\n",
    "aseg = tacrhie_training.subjects[0].get_aseg()\n",
    "aseg_aparc = tacrhie_training.subjects[0].get_aseg_aparc()\n",
    "\n",
    "plot_side_by_side([sitk.GetArrayViewFromImage(norm), sitk.GetArrayViewFromImage(aseg), sitk.GetArrayViewFromImage(aseg_aparc)], [\"norm\", \"aseg\", \"aseg+aparc\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aseg + aparc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(dataset_specific_directory, \"tacrhie6_aseg_aparc.keras\")\n",
    "\n",
    "num_channels = 1\n",
    "batch_size = 2\n",
    "\n",
    "input_shape = (*next(training.take(1).as_numpy_iterator())[0].shape, num_channels)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_creator = lambda : unet_compiled(input_size=input_shape, n_filters=24, n_classes=len(labels_dictionary) + 1, learning_rate=0.0001)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50),\n",
    "]\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\" : callbacks_list,\n",
    "    \"training_data\" : training.batch(batch_size),\n",
    "    \"validation_data\" : validation.batch(batch_size),\n",
    "    'epochs' : 500\n",
    "}\n",
    "\n",
    "seed_all()\n",
    "best_model_path, history = run_training(unet_creator, fit_params, 10, dataset_specific_directory, \"tacrhie_aseg_aparc\")\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    os.remove(model_file)\n",
    "    \n",
    "shutil.copy(best_model_path, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = keras.saving.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = unet.predict(testing.batch(1))\n",
    "predicted_labels = prediction.argmax(axis=-1).astype(np.uint8)\n",
    "\n",
    "shape = (predicted_labels.shape[0], 256, 256, 256)\n",
    "begin = (np.array(shape) - predicted_labels.shape) // 2\n",
    "begin[0] = 0\n",
    "\n",
    "results = mia_utils.embed_tensor(predicted_labels, shape, begin)\n",
    "\n",
    "mia_utils.writeImagesArray(results, inference_directory, \n",
    "                           lambda x : f\"{tacrhie_testing.subjects[x].number}.nii.gz\",\n",
    "                            lambda x : tacrhie_testing.subjects[x].get_aseg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_utils.interactive_display(predicted_labels[0], (1, 256), title=\"TACR-HIE aseg+aparc Inference\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_array = next(testing.take(1).as_numpy_iterator())[1]\n",
    "\n",
    "mia_utils.interactive_display(truth_array, (1, 256), title=\"TACR-HIE aseg+aparc  Ground truth\", cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = sitk.GetImageFromArray(predicted_labels[0])\n",
    "truth_image = sitk.GetImageFromArray(truth_array)\n",
    "\n",
    "eval = mia_eval.evaluateImage(predicted_image, truth_image, labels_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_eval.createRecord(\"tarc_aseg_aparc\", eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
